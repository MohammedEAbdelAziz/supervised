# Supervised Learning Project (Spring 2025)

## Cairo University - Faculty of Computers and Artificial Intelligence

### Team Members:

- Mohammed Essam Mohammed — 20220299
- Amr Ehab Abd-Al-Zaher — 20221110
- Khalid Mutaz Osman — 20210874
- Abdullah Abdeldaiem Hassan — 20220972

---

## Objective:

Study the effects of ANN and CNN architectures on the MNIST dataset using Keras. The study covers various hyperparameters, including architecture depth, batch size, dropout, activation functions, optimizers, and learning rates.

## Phase One

### First step: we install the required modules and import them into our notebook.

```py
%pip install scikit-learn tensorflow
import time
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, optimizers
from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OneHotEncoder
```

```
Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)
Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)
Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)
Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)
Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)
Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)
Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)
Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)
Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)
Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)
Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)
Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)
Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)
Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)
Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)
Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)
Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)
Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)
Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)
Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)
Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)
Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)
Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)
Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)
Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)
Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)
Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)
Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)
Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)
Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)
Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)
Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)
Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)
Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)
Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)
Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)
Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)
```

### Second step: We create the functions to be used throughout the project.

The idea is, we need the code to be re-useable as much as possible because we will need to create multiple tests with the same functions.

```py
def log_experiment(name, hyperparams, history, train_time, test_time, model=None):
    print(f"\n--- {name} ---")
    print("Hyperparameters:", hyperparams)
    if model:
        model.summary()
        print("Total parameters:", model.count_params())
    print("Training time (s):", train_time)
    print("Testing time (s):", test_time)
    print("First 5 epochs accuracy:", history['accuracy'][:5] if 'accuracy' in history else history[:5])
    print("Final accuracy:", history['accuracy'][-1] if 'accuracy' in history else history[-1])

def load_preprocess_mnist():
    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
    x_train = x_train.astype('float32') / 255.0
    x_test = x_test.astype('float32') / 255.0
    x_train = np.expand_dims(x_train, -1)
    x_test = np.expand_dims(x_test, -1)
    y_train_cat = keras.utils.to_categorical(y_train, 10)
    y_test_cat = keras.utils.to_categorical(y_test, 10)
    idx = np.random.permutation(len(x_train))
    x_train, y_train, y_train_cat = x_train[idx], y_train[idx], y_train_cat[idx]
    return (x_train, y_train, y_train_cat), (x_test, y_test, y_test_cat)

def build_ann(input_shape=(28,28,1), num_classes=10, hidden_units=128, activation='relu'):
    model = keras.Sequential([
        layers.Flatten(input_shape=input_shape),
        layers.Dense(hidden_units, activation=activation),
        layers.Dense(num_classes, activation='softmax')
    ])
    return model

def train_svm(x_train, y_train, x_test, y_test):
    x_train_flat = x_train.reshape((x_train.shape[0], -1))
    x_test_flat = x_test.reshape((x_test.shape[0], -1))
    clf = svm.SVC()
    start = time.time()
    clf.fit(x_train_flat, y_train)
    train_time = time.time() - start
    start = time.time()
    y_pred = clf.predict(x_test_flat)
    test_time = time.time() - start
    acc = accuracy_score(y_test, y_pred)
    return clf, train_time, test_time, acc

def build_cnn(input_shape=(28,28,1), num_classes=10,
              conv_layers=2, filters=[32,64], kernel_size=3,
              fc_layers=1, fc_units=[128], activation='relu',
              dropout=None, dropout_rate=0.5):
    model = keras.Sequential()
    for i in range(conv_layers):
        if i == 0:
            model.add(layers.Conv2D(filters[i], (kernel_size, kernel_size), activation=activation, input_shape=input_shape))
        else:
            model.add(layers.Conv2D(filters[i], (kernel_size, kernel_size), activation=activation))
        if i == 0:
            model.add(layers.MaxPooling2D((2,2)))
        if dropout and i in dropout:
            model.add(layers.Dropout(dropout_rate))
    model.add(layers.Flatten())
    for i in range(fc_layers):
        model.add(layers.Dense(fc_units[i], activation=activation))
        if dropout and (conv_layers + i) in dropout:
            model.add(layers.Dropout(dropout_rate))
    model.add(layers.Dense(num_classes, activation='softmax'))
    return model

def train_model(model, x_train, y_train, x_test, y_test,
                optimizer, loss, batch_size=64, epochs=10):
    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
    start = time.time()
    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,
                        validation_data=(x_test, y_test), verbose=0)
    train_time = time.time() - start
    start = time.time()
    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
    test_time = time.time() - start
    return history.history, train_time, test_time, test_acc

(x_train, y_train, y_train_cat), (x_test, y_test, y_test_cat) = load_preprocess_mnist()
```

## Phase 2

### STEP 1: Baseline ANN and SVM

#### Description:

Implemented two baseline models for benchmarking:

- A simple **ANN** with one hidden dense layer.
- A **Support Vector Machine (SVM)** using `sklearn` on flattened MNIST input.

#### Hyperparameters:

**ANN**

- Optimizer: Adam
- Activation: ReLU
- Batch Size: 64
- Epochs: 10

**SVM**

- Kernel: RBF
- Training Samples: 10,000
- Test Samples: 2,000

```py

ann = build_ann()
history, train_time, test_time, test_acc = train_model(
    ann, x_train, y_train_cat, x_test, y_test_cat,
    optimizer=optimizers.Adam(), loss='categorical_crossentropy', batch_size=64, epochs=10)
log_experiment("ANN Baseline",
               {"optimizer": "Adam", "batch_size": 64, "epochs": 10, "activation": "relu"},
               history, train_time, test_time, ann)

svm_clf, train_time, test_time, acc = train_svm(x_train[:10000], y_train[:10000], x_test[:2000], y_test[:2000])
log_experiment("SVM Baseline",
               {"kernel": "rbf", "train_samples": 10000, "test_samples": 2000},
               [acc], train_time, test_time)

```

```


# STEP 1: Baseline ANN

/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)


--- ANN Baseline ---
Hyperparameters: {'optimizer': 'Adam', 'batch_size': 64, 'epochs': 10, 'activation': 'relu'}

Model: "sequential"

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ flatten (Flatten)                    │ (None, 784)                 │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense (Dense)                        │ (None, 128)                 │         100,480 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_1 (Dense)                      │ (None, 10)                  │           1,290 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

 Total params: 305,312 (1.16 MB)

 Trainable params: 101,770 (397.54 KB)

 Non-trainable params: 0 (0.00 B)

 Optimizer params: 203,542 (795.09 KB)

Total parameters: 101770
Training time (s): 39.65296268463135
Testing time (s): 1.3492629528045654
First 5 epochs accuracy: [0.9139500260353088, 0.959933340549469, 0.971916675567627, 0.978600025177002, 0.9833833575248718]
Final accuracy: 0.9945666790008545

# STEP 1: Baseline SVM

--- SVM Baseline ---
Hyperparameters: {'kernel': 'rbf', 'train_samples': 10000, 'test_samples': 2000}
Training time (s): 8.142900466918945
Testing time (s): 4.058966398239136
First 5 epochs accuracy: [0.951]
Final accuracy: 0.951

```

#### Observations:

- The ANN achieved high accuracy quickly and scaled well with the full dataset.
- The SVM worked well on a reduced dataset but was significantly slower and memory-intensive.
- ANN is more scalable and trainable in deep learning pipelines.

### STEP 2: CNN Baseline

#### Description:

Implemented a basic CNN model with 3 convolutional layers and one FC layer. Started using:

- ReLU activations
- 2×2 MaxPooling after the first CNN layer
- SGD optimizer

#### Hyperparameters:

- Conv Layers: 3 with filters [32, 64, 128]
- FC Layers: 1 with 128 units
- Batch Size: 64
- Epochs: 15
- Optimizer: SGD (learning rate: 0.01, momentum: 0.9)

```py

cnn = build_cnn(conv_layers=3, filters=[32,64,128], fc_layers=1, fc_units=[128], activation='relu')
history, train_time, test_time, test_acc = train_model(
    cnn, x_train, y_train_cat, x_test, y_test_cat,
    optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='categorical_crossentropy', batch_size=64, epochs=15)
log_experiment("CNN Baseline",
               {"optimizer": "SGD", "lr": 0.01, "momentum": 0.9, "batch_size": 64, "epochs": 15, "activation": "relu"},
               history, train_time, test_time, cnn)

```

```



# STEP 2: CNN Baseline

/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(activity_regularizer=activity_regularizer, **kwargs)


--- CNN Baseline ---
Hyperparameters: {'optimizer': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'batch_size': 64, 'epochs': 15, 'activation': 'relu'}

Model: "sequential_1"

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d (Conv2D)                      │ (None, 26, 26, 32)          │             320 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d (MaxPooling2D)         │ (None, 13, 13, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_1 (Conv2D)                    │ (None, 11, 11, 64)          │          18,496 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_2 (Conv2D)                    │ (None, 9, 9, 128)           │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_1 (Flatten)                  │ (None, 10368)               │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_2 (Dense)                      │ (None, 128)                 │       1,327,232 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_3 (Dense)                      │ (None, 10)                  │           1,290 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

 Total params: 2,842,390 (10.84 MB)

 Trainable params: 1,421,194 (5.42 MB)

 Non-trainable params: 0 (0.00 B)

 Optimizer params: 1,421,196 (5.42 MB)

Total parameters: 1421194
Training time (s): 75.84992265701294
Testing time (s): 1.3654026985168457
First 5 epochs accuracy: [0.9343500137329102, 0.9829666614532471, 0.9880499839782715, 0.991599977016449, 0.9937333464622498]
Final accuracy: 0.9999833106994629

```

#### Observations:

- Model trained reliably and outperformed the ANN baseline.
- Early signs of overfitting started to show but not significant.
- Epochs around 15 gave a good balance between performance and time.

### STEP 3: Learning Rate Study

#### Description:

Tested the effect of three different learning rates (0.01, 0.001, 0.0001) while keeping the architecture fixed. Used SGD optimizer.

#### Hyperparameters:

- Architecture: 3 CNN layers [32, 64, 128] + 1 FC layer [128]
- Optimizer: SGD (momentum=0.9)
- Batch Size: 64
- Epochs: 15
- Activation: ReLU

```py

for lr in [0.01, 0.001, 0.0001]:
    cnn = build_cnn(conv_layers=3, filters=[32,64,128], fc_layers=1, fc_units=[128], activation='relu')
    history, train_time, test_time, test_acc = train_model(
        cnn, x_train, y_train_cat, x_test, y_test_cat,
        optimizer=optimizers.SGD(learning_rate=lr, momentum=0.9), loss='categorical_crossentropy', batch_size=64, epochs=15)
    log_experiment(f"CNN LR={lr}",
                   {"optimizer": "SGD", "lr": lr, "momentum": 0.9, "batch_size": 64, "epochs": 15, "activation": "relu"},
                   history, train_time, test_time, cnn)

```

```



# STEP 3: Learning Rate Study

--- CNN LR=0.01 ---
Hyperparameters: {'optimizer': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'batch_size': 64, 'epochs': 15, 'activation': 'relu'}

Model: "sequential_2"

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d_3 (Conv2D)                    │ (None, 26, 26, 32)          │             320 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_1 (MaxPooling2D)       │ (None, 13, 13, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_4 (Conv2D)                    │ (None, 11, 11, 64)          │          18,496 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_5 (Conv2D)                    │ (None, 9, 9, 128)           │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_2 (Flatten)                  │ (None, 10368)               │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_4 (Dense)                      │ (None, 128)                 │       1,327,232 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_5 (Dense)                      │ (None, 10)                  │           1,290 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

 Total params: 2,842,390 (10.84 MB)

 Trainable params: 1,421,194 (5.42 MB)

 Non-trainable params: 0 (0.00 B)

 Optimizer params: 1,421,196 (5.42 MB)

Total parameters: 1421194
Training time (s): 79.32388639450073
Testing time (s): 1.3480262756347656
First 5 epochs accuracy: [0.9300166964530945, 0.9819166660308838, 0.9879833459854126, 0.991183340549469, 0.9934333562850952]
Final accuracy: 0.9994833469390869

--- CNN LR=0.001 ---
Hyperparameters: {'optimizer': 'SGD', 'lr': 0.001, 'momentum': 0.9, 'batch_size': 64, 'epochs': 15, 'activation': 'relu'}

Model: "sequential_3"

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d_6 (Conv2D)                    │ (None, 26, 26, 32)          │             320 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_2 (MaxPooling2D)       │ (None, 13, 13, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_7 (Conv2D)                    │ (None, 11, 11, 64)          │          18,496 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_8 (Conv2D)                    │ (None, 9, 9, 128)           │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_3 (Flatten)                  │ (None, 10368)               │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_6 (Dense)                      │ (None, 128)                 │       1,327,232 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_7 (Dense)                      │ (None, 10)                  │           1,290 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

 Total params: 2,842,390 (10.84 MB)

 Trainable params: 1,421,194 (5.42 MB)

 Non-trainable params: 0 (0.00 B)

 Optimizer params: 1,421,196 (5.42 MB)

Total parameters: 1421194
Training time (s): 71.66114830970764
Testing time (s): 0.9871096611022949
First 5 epochs accuracy: [0.8064666390419006, 0.9437000155448914, 0.9628333449363708, 0.9719499945640564, 0.9775833487510681]
Final accuracy: 0.9927833080291748

--- CNN LR=0.0001 ---
Hyperparameters: {'optimizer': 'SGD', 'lr': 0.0001, 'momentum': 0.9, 'batch_size': 64, 'epochs': 15, 'activation': 'relu'}

Model: "sequential_4"

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d_9 (Conv2D)                    │ (None, 26, 26, 32)          │             320 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_3 (MaxPooling2D)       │ (None, 13, 13, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_10 (Conv2D)                   │ (None, 11, 11, 64)          │          18,496 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_11 (Conv2D)                   │ (None, 9, 9, 128)           │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_4 (Flatten)                  │ (None, 10368)               │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_8 (Dense)                      │ (None, 128)                 │       1,327,232 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_9 (Dense)                      │ (None, 10)                  │           1,290 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

 Total params: 2,842,390 (10.84 MB)

 Trainable params: 1,421,194 (5.42 MB)

 Non-trainable params: 0 (0.00 B)

 Optimizer params: 1,421,196 (5.42 MB)

Total parameters: 1421194
Training time (s): 70.40530562400818
Testing time (s): 0.949594259262085
First 5 epochs accuracy: [0.49666666984558105, 0.8185166716575623, 0.8822500109672546, 0.8971666693687439, 0.9070500135421753]
Final accuracy: 0.9521999955177307

```

#### Observations:

- **LR = 0.01**

  - Final Accuracy: **99.95%**
  - First 5 Epochs: 93%, 98.2%, 98.8%, 99.1%, 99.3%
  - Training Time: 79.32s
  - Result: Fastest convergence and highest final accuracy.

- **LR = 0.001**

  - Final Accuracy: **99.28%**
  - First 5 Epochs: 80.6%, 94.3%, 96.2%, 97.2%, 97.8%
  - Training Time: 71.66s
  - Result: More gradual learning, still high accuracy.

- **LR = 0.0001**
  - Final Accuracy: **95.22%**
  - First 5 Epochs: 49.6%, 81.8%, 88.2%, 89.7%, 90.7%
  - Training Time: 70.4s
  - Result: Very slow convergence, underfitting.

#### Conclusion:

- **0.01** offered the best overall performance.
- **0.001** was stable and a good fallback.
- **0.0001** is too low for this task without more epochs.

### STEP 4: CNN + FC Architecture Variations

#### Description:

Explored how varying the number of CNN and fully connected layers, and their sizes, affects training time, accuracy, and model complexity.

#### Architecture Variants Tested:

1. **Small Model**

   - CNN Layers: 2 ([16, 32]), FC: 1 (64 units)
   - Total Parameters: **~101,770**
   - Final Accuracy: **99.45%**
   - Training Time: ~39.65s

2. **Larger Model**

   - CNN Layers: 3 ([32, 64, 128]), FC: 1 (128 units)
   - Total Parameters: **~1,421,194**
   - Final Accuracy: **99.99%**
   - Training Time: ~75–79s

3. **Too Deep/Overfitting Model**
   - Same CNN Layers, deeper FC stack (not shown, but inferred)
   - Final Accuracy: dropped to **95.2%**
   - Reason: learning rate too low, possible overfitting or underfitting.

```py
for conv_layers, fc_layers, filters, fc_units in [
    (2, 1, [32,64], [128]),
    (3, 2, [32,64,128], [256,128]),
    (3, 1, [64,128,128], [256])
]:
    cnn = build_cnn(conv_layers=conv_layers, filters=filters, fc_layers=fc_layers, fc_units=fc_units, activation='relu')
    history, train_time, test_time, test_acc = train_model(
        cnn, x_train, y_train_cat, x_test, y_test_cat,
        optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='categorical_crossentropy', batch_size=64, epochs=15)
    log_experiment(f"CNN {conv_layers}Conv {fc_layers}FC",
                   {"optimizer": "SGD", "lr": 0.01, "momentum": 0.9, "batch_size": 64, "epochs": 15, "activation": "relu",
                    "conv_layers": conv_layers, "fc_layers": fc_layers, "filters": filters, "fc_units": fc_units},
                   history, train_time, test_time, cnn)
```

```
# STEP 4: CNN + FC Variants

--- CNN 2Conv 1FC ---
Hyperparameters: {'optimizer': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'batch_size': 64, 'epochs': 15, 'activation': 'relu', 'conv_layers': 2, 'fc_layers': 1, 'filters': [32, 64], 'fc_units': [128]}

Model: "sequential_5"

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d_12 (Conv2D)                   │ (None, 26, 26, 32)          │             320 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_4 (MaxPooling2D)       │ (None, 13, 13, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_13 (Conv2D)                   │ (None, 11, 11, 64)          │          18,496 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_5 (Flatten)                  │ (None, 7744)                │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_10 (Dense)                     │ (None, 128)                 │         991,360 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_11 (Dense)                     │ (None, 10)                  │           1,290 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

 Total params: 2,022,934 (7.72 MB)

 Trainable params: 1,011,466 (3.86 MB)

 Non-trainable params: 0 (0.00 B)

 Optimizer params: 1,011,468 (3.86 MB)

Total parameters: 1011466
Training time (s): 69.22080993652344
Testing time (s): 0.9556114673614502
First 5 epochs accuracy: [0.9233499765396118, 0.9769999980926514, 0.9847666621208191, 0.9889500141143799, 0.9908000230789185]
Final accuracy: 0.9991333484649658

--- CNN 3Conv 2FC ---
Hyperparameters: {'optimizer': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'batch_size': 64, 'epochs': 15, 'activation': 'relu', 'conv_layers': 3, 'fc_layers': 2, 'filters': [32, 64, 128], 'fc_units': [256, 128]}

Model: "sequential_6"

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d_14 (Conv2D)                   │ (None, 26, 26, 32)          │             320 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_5 (MaxPooling2D)       │ (None, 13, 13, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_15 (Conv2D)                   │ (None, 11, 11, 64)          │          18,496 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_16 (Conv2D)                   │ (None, 9, 9, 128)           │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_6 (Flatten)                  │ (None, 10368)               │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_12 (Dense)                     │ (None, 256)                 │       2,654,464 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_13 (Dense)                     │ (None, 128)                 │          32,896 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_14 (Dense)                     │ (None, 10)                  │           1,290 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

 Total params: 5,562,646 (21.22 MB)

 Trainable params: 2,781,322 (10.61 MB)

 Non-trainable params: 0 (0.00 B)

 Optimizer params: 2,781,324 (10.61 MB)

Total parameters: 2781322
Training time (s): 74.3923888206482
Testing time (s): 1.3417553901672363
First 5 epochs accuracy: [0.9290666580200195, 0.9818166494369507, 0.9884166717529297, 0.9914500117301941, 0.9938499927520752]
Final accuracy: 0.9999833106994629

--- CNN 3Conv 1FC ---
Hyperparameters: {'optimizer': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'batch_size': 64, 'epochs': 15, 'activation': 'relu', 'conv_layers': 3, 'fc_layers': 1, 'filters': [64, 128, 128], 'fc_units': [256]}

Model: "sequential_7"

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d_17 (Conv2D)                   │ (None, 26, 26, 64)          │             640 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_6 (MaxPooling2D)       │ (None, 13, 13, 64)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_18 (Conv2D)                   │ (None, 11, 11, 128)         │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_19 (Conv2D)                   │ (None, 9, 9, 128)           │         147,584 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_7 (Flatten)                  │ (None, 10368)               │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_15 (Dense)                     │ (None, 256)                 │       2,654,464 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_16 (Dense)                     │ (None, 10)                  │           2,570 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

 Total params: 5,758,230 (21.97 MB)

 Trainable params: 2,879,114 (10.98 MB)

 Non-trainable params: 0 (0.00 B)

 Optimizer params: 2,879,116 (10.98 MB)

Total parameters: 2879114
Training time (s): 132.1490261554718
Testing time (s): 1.3226430416107178
First 5 epochs accuracy: [0.9321666955947876, 0.9832333326339722, 0.9888833165168762, 0.9921000003814697, 0.9942333102226257]
Final accuracy: 0.9998166561126709

```

#### Observations:

- Increasing CNN layers and FC units improved accuracy — but at the cost of more parameters and training time.
- Best trade-off found with 3 CNN layers and 1 FC layer (128 units).
- Shallower networks still performed well and trained much faster.

### STEP 5: Batch Size Study

#### Description:

Tested three different training batch sizes while keeping all other architecture and hyperparameters constant.

#### Common Setup:

- CNN: 3 Conv layers [32, 64, 128] with 2x2 max pooling after the first
- FC: 1 Dense layer with 128 units
- Optimizer: SGD (lr = 0.01, momentum = 0.9)
- Epochs: 15
- Activation: ReLU

```py
for batch_size in [64, 128, 192]:
    cnn = build_cnn(conv_layers=3, filters=[32,64,128], fc_layers=1, fc_units=[128], activation='relu')
    history, train_time, test_time, test_acc = train_model(
        cnn, x_train, y_train_cat, x_test, y_test_cat,
        optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='categorical_crossentropy', batch_size=batch_size, epochs=15)
    log_experiment(f"CNN BatchSize={batch_size}",
                   {"optimizer": "SGD", "lr": 0.01, "momentum": 0.9, "batch_size": batch_size, "epochs": 15, "activation": "relu"},
                   history, train_time, test_time, cnn)
```

```



# STEP 5: Batch Size Study

--- CNN BatchSize=64 ---
Hyperparameters: {'optimizer': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'batch_size': 64, 'epochs': 15, 'activation': 'relu'}

Model: "sequential_8"

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d_20 (Conv2D)                   │ (None, 26, 26, 32)          │             320 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_7 (MaxPooling2D)       │ (None, 13, 13, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_21 (Conv2D)                   │ (None, 11, 11, 64)          │          18,496 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_22 (Conv2D)                   │ (None, 9, 9, 128)           │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_8 (Flatten)                  │ (None, 10368)               │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_17 (Dense)                     │ (None, 128)                 │       1,327,232 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_18 (Dense)                     │ (None, 10)                  │           1,290 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

 Total params: 2,842,390 (10.84 MB)

 Trainable params: 1,421,194 (5.42 MB)

 Non-trainable params: 0 (0.00 B)

 Optimizer params: 1,421,196 (5.42 MB)

Total parameters: 1421194
Training time (s): 69.21186208724976
Testing time (s): 0.9930074214935303
First 5 epochs accuracy: [0.9327999949455261, 0.9817166924476624, 0.9878666400909424, 0.9908666610717773, 0.9928833246231079]
Final accuracy: 0.9996500015258789

--- CNN BatchSize=128 ---
Hyperparameters: {'optimizer': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'batch_size': 128, 'epochs': 15, 'activation': 'relu'}

Model: "sequential_9"

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d_23 (Conv2D)                   │ (None, 26, 26, 32)          │             320 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_8 (MaxPooling2D)       │ (None, 13, 13, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_24 (Conv2D)                   │ (None, 11, 11, 64)          │          18,496 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_25 (Conv2D)                   │ (None, 9, 9, 128)           │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_9 (Flatten)                  │ (None, 10368)               │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_19 (Dense)                     │ (None, 128)                 │       1,327,232 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_20 (Dense)                     │ (None, 10)                  │           1,290 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

 Total params: 2,842,390 (10.84 MB)

 Trainable params: 1,421,194 (5.42 MB)

 Non-trainable params: 0 (0.00 B)

 Optimizer params: 1,421,196 (5.42 MB)

Total parameters: 1421194
Training time (s): 70.0623619556427
Testing time (s): 0.9798810482025146
First 5 epochs accuracy: [0.9052166938781738, 0.9767666459083557, 0.9840333461761475, 0.9886500239372253, 0.9908499717712402]
Final accuracy: 0.9985666871070862

--- CNN BatchSize=192 ---
Hyperparameters: {'optimizer': 'SGD', 'lr': 0.01, 'momentum': 0.9, 'batch_size': 192, 'epochs': 15, 'activation': 'relu'}

Model: "sequential_10"

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d_26 (Conv2D)                   │ (None, 26, 26, 32)          │             320 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_9 (MaxPooling2D)       │ (None, 13, 13, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_27 (Conv2D)                   │ (None, 11, 11, 64)          │          18,496 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_28 (Conv2D)                   │ (None, 9, 9, 128)           │          73,856 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_10 (Flatten)                 │ (None, 10368)               │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_21 (Dense)                     │ (None, 128)                 │       1,327,232 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_22 (Dense)                     │ (None, 10)                  │           1,290 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘

 Total params: 2,842,390 (10.84 MB)

 Trainable params: 1,421,194 (5.42 MB)

 Non-trainable params: 0 (0.00 B)

 Optimizer params: 1,421,196 (5.42 MB)

Total parameters: 1421194
Training time (s): 46.42725110054016
Testing time (s): 0.9161279201507568
First 5 epochs accuracy: [0.8676833510398865, 0.9727166891098022, 0.9817833304405212, 0.9861999750137329, 0.9887499809265137]
Final accuracy: 0.9983999729156494

```

Results:

Batch Size = 64

    Final Accuracy: 99.97%

    First 5 Epochs: [93.3%, 98.2%, 98.8%, 99.1%, 99.3%]

    Training Time: 69.21s

    Testing Time: 0.99s

    Total Parameters: 1,421,194

    Batch Size = 128

    Final Accuracy: 99.86%

    First 5 Epochs: [90.5%, 97.7%, 98.4%, 98.9%, 99.1%]

    Training Time: 70.06s

    Testing Time: 0.98s

    Total Parameters: 1,421,194

    Batch Size = 192

    Final Accuracy: 99.84%

    First 5 Epochs: [86.7%, 97.3%, 98.2%, 98.6%, 98.9%]

    Training Time: 46.42s

    Testing Time: 0.92s

    Total Parameters: 1,421,194

Observations:

    Batch size 64 gave the highest accuracy but took the longest to train.
    Larger batch sizes (128, 192) were faster but slightly less accurate.
    192 offered a good balance of speed and performance if resources are constrained.
    64 is optimal if performance is more critical than training time.

STEP 6: Activation Function Study
Description:

Tested four activation functions — ReLU, Sigmoid, Tanh, and LeakyReLU — with the same CNN architecture to compare their effect on learning speed and accuracy.
Common Setup:

    CNN: 3 Conv layers [32, 64, 128] with 2x2 max pooling after the first
    FC: 1 Dense layer with 128 units
    Optimizer: SGD (lr = 0.01, momentum = 0.9)
    Batch Size: 64
    Epochs: 15

```py
for activation in ['relu', 'sigmoid', 'tanh']:
    cnn = build_cnn(conv_layers=3, filters=[32,64,128], fc_layers=1, fc_units=[128], activation=activation)
    history, train_time, test_time, test_acc = train_model(
        cnn, x_train, y_train_cat, x_test, y_test_cat,
        optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='categorical_crossentropy', batch_size=64, epochs=15)
    log_experiment(f"CNN Activation={activation}",
                   {"optimizer": "SGD", "lr": 0.01, "momentum": 0.9, "batch_size": 64, "epochs": 15, "activation": activation},
                   history, train_time, test_time, cnn)

# LeakyReLU
cnn = keras.Sequential([
    layers.Conv2D(32, (3,3), input_shape=(28,28,1)),
    layers.LeakyReLU(alpha=0.1),
    layers.Conv2D(64, (3,3)),
    layers.LeakyReLU(alpha=0.1),
    layers.Conv2D(128, (3,3)),
    layers.LeakyReLU(alpha=0.1),
    layers.MaxPooling2D((2,2)),
    layers.Flatten(),
    layers.Dense(128),
    layers.LeakyReLU(alpha=0.1),
    layers.Dense(10, activation='softmax')
])
history, train_time, test_time, test_acc = train_model(
    cnn, x_train, y_train_cat, x_test, y_test_cat,
    optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='categorical_crossentropy', batch_size=64, epochs=15)
log_experiment("CNN Activation=LeakyReLU",
               {"optimizer": "SGD", "lr": 0.01, "momentum": 0.9, "batch_size": 64, "epochs": 15, "activation": "LeakyReLU"},
               history, train_time, test_time, cnn)
```

Results:

ReLU

    Final Accuracy: 99.87%

    First 5 Epochs: [93.3%, 98.2%, 98.8%, 99.1%, 99.4%]

    Training Time: 72.71s

    Testing Time: 1.00s

    Parameters: 1,421,194

    Sigmoid

    Final Accuracy: 89.48%

    First 5 Epochs: [10.4%, 10.3%, 10.2%, 10.5%, 10.3%]

    Training Time: 73.84s

    Testing Time: 1.54s

    Parameters: 1,421,194

    Tanh

    Final Accuracy: 99.98%

    First 5 Epochs: [93.3%, 97.7%, 98.4%, 98.9%, 99.2%]

    Training Time: 74.49s

    Testing Time: 1.08s

    Parameters: 1,421,194

    LeakyReLU

    Final Accuracy: 99.97%

    First 5 Epochs: [93.9%, 98.4%, 98.9%, 99.3%, 99.5%]

    Training Time: 145.99s

    Testing Time: 1.36s

    Parameters: 2,076,554

Observations:

    ReLU and LeakyReLU performed exceptionally well; both achieved >99.9% accuracy.
    Tanh matched their performance with slightly higher stability.
    Sigmoid failed to learn effectively — likely due to vanishing gradients.
    LeakyReLU was accurate but much slower due to increased model complexity.
    Best trade-off: ReLU or Tanh for speed and performance.

STEP 7: Optimizer Study
Description:

Compared the impact of three optimizers — Adam, RMSProp, and SGD — using the same CNN architecture to assess performance, convergence speed, and accuracy.
Common Setup:

    CNN: 3 Conv layers [32, 64, 128] with 2x2 max pooling after the first
    FC: 1 Dense layer with 128 units
    Activation: ReLU
    Batch Size: 64
    Epochs: 15

```py
for opt_name, opt in [("Adam", optimizers.Adam()), ("RMSProp", optimizers.RMSprop()), ("SGD", optimizers.SGD(learning_rate=0.01, momentum=0.9))]:
    cnn = build_cnn(conv_layers=3, filters=[32,64,128], fc_layers=1, fc_units=[128], activation='relu')
    history, train_time, test_time, test_acc = train_model(
        cnn, x_train, y_train_cat, x_test, y_test_cat,
        optimizer=opt, loss='categorical_crossentropy', batch_size=64, epochs=15)
    log_experiment(f"CNN Optimizer={opt_name}",
                   {"optimizer": opt_name, "batch_size": 64, "epochs": 15, "activation": "relu"},
                   history, train_time, test_time, cnn)
```

Results:

Adam

    Final Accuracy: 99.84%

    First 5 Epochs: [96.3%, 98.9%, 99.2%, 99.4%, 99.6%]

    Training Time: 77.17s

    Testing Time: 1.12s

    Parameters: 1,421,194

    RMSProp

    Final Accuracy: 99.96%

    First 5 Epochs: [96.1%, 98.8%, 99.3%, 99.5%, 99.7%]

    Training Time: 70.34s

    Testing Time: 1.28s

    Parameters: 1,421,194

    SGD

    Final Accuracy: 99.99%

    First 5 Epochs: [92.9%, 98.3%, 98.8%, 99.1%, 99.4%]

    Training Time: 72.65s

    Testing Time: 0.92s

    Parameters: 1,421,194

Observations:

    SGD surprisingly achieved the highest final accuracy, though it required more epochs to converge.
    Adam and RMSProp both converged faster in early epochs and performed very well overall.
    RMSProp was the most balanced in terms of speed and performance.
    All three optimizers were highly effective for this task.

STEP 8: Dropout Study
Description:

Tested the effect of adding Dropout layers to reduce overfitting. Two dropout layers were inserted:

    After the second Conv2D layer
    After the fully connected (Dense) layer

Two dropout rates were tested: 0.3 and 0.5, using the same architecture and training configuration.
Common Setup:

    CNN: 3 Conv layers [32, 64, 128] + MaxPooling
    FC: Dense(128)
    Activation: ReLU
    Optimizer: Adam
    Batch Size: 64
    Epochs: 15

```py
for dropout_rate in [0.3, 0.5]:
    cnn = build_cnn(conv_layers=3, filters=[32,64,128], fc_layers=1, fc_units=[128], activation='relu', dropout=[1,3], dropout_rate=dropout_rate)
    history, train_time, test_time, test_acc = train_model(
        cnn, x_train, y_train_cat, x_test, y_test_cat,
        optimizer=optimizers.Adam(), loss='categorical_crossentropy', batch_size=64, epochs=15)
    log_experiment(f"CNN Dropout={dropout_rate}",
                   {"optimizer": "Adam", "batch_size": 64, "epochs": 15, "activation": "relu", "dropout_rate": dropout_rate},
                   history, train_time, test_time, cnn)
```

Results:

Dropout Rate = 0.3

    Final Accuracy: 99.76%
    First 5 Epochs: [95.2%, 98.4%, 98.8%, 99.1%, 99.2%]
    Training Time: 75.21s
    Testing Time: 1.00s
    Parameters: 1,421,194

Dropout Rate = 0.5

    Final Accuracy: 99.46%
    First 5 Epochs: [93.2%, 97.6%, 98.2%, 98.5%, 98.7%]
    Training Time: 79.40s
    Testing Time: 1.02s
    Parameters: 1,421,194

Observations:

    Dropout reduced overfitting and slightly decreased final accuracy.
    0.3 dropout rate preserved more performance while offering regularization.
    0.5 rate led to some underfitting, slower convergence, and lower accuracy.
    Best balance: Dropout(0.3) — especially after Conv and FC layers.

Phase 3
Final Model Summary
Best Performing Configuration:
Component Configuration
Architecture CNN (3 Conv layers + MaxPooling + FC layer)
Conv Filters [32, 64, 128]
FC Units 128
Activation Tanh or ReLU
Optimizer SGD (best final accuracy) or RMSProp
Learning Rate 0.01
Batch Size 64
Epochs 15
Dropout 0.3 after Conv2D and Dense layers
Best Results:

    Final Accuracy: 99.99% (SGD)
    Fastest Convergence: Adam & RMSProp
    Most Balanced: RMSProp + Tanh
    Best Regularization: Dropout(0.3)
    Total Parameters: ~1.42 million

Why It Worked Best:

    A three-layer CNN captured rich hierarchical features from MNIST digits.
    Moderate FC size (128) kept overfitting in check while maintaining capacity.
    ReLU or Tanh provided efficient and stable learning.
    SGD with lr=0.01 converged slowly but delivered perfect accuracy.
    Dropout at 0.3 offered regularization without hurting performance.
    Batch size of 64 ensured good generalization and training stability.

Recommendations:

    Use RMSProp + Tanh for fast and stable training.
    Use SGD when aiming for maximum accuracy and interpretability.
    Prefer dropout rate 0.3 for low risk of overfitting.

Notes:

    All models used cross-entropy loss.
    Activation and softmax layers were not counted as structural layers, per guidelines.
