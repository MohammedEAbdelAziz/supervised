{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cabb341a",
      "metadata": {
        "id": "cabb341a"
      },
      "source": [
        "# Supervised Learning Project (Spring 2025)\n",
        "\n",
        "## Cairo University - Faculty of Computers and Artificial Intelligence\n",
        "\n",
        "### Team Members:\n",
        "- Mohammed Essam Mohammed — 20220299\n",
        "- Amr Ehab Abd-Al-Zaher — 20221110\n",
        "- Khalid Mutaz Osman — 20210874\n",
        "- Abdullah Abdeldaiem Hassan — 20220972\n",
        "\n",
        "---\n",
        "\n",
        "## Objective:\n",
        "Study the effects of ANN and CNN architectures on the MNIST dataset using Keras. The study covers various hyperparameters, including architecture depth, batch size, dropout, activation functions, optimizers, and learning rates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "joDKUt8urMaY",
      "metadata": {
        "id": "joDKUt8urMaY"
      },
      "source": [
        "## Phase One"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-HTfUxZNlJnq",
      "metadata": {
        "id": "-HTfUxZNlJnq"
      },
      "source": [
        "### First step: we install the required modules and import them into our notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cd4ff9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cd4ff9a",
        "outputId": "4e65cd5d-5386-4c10-fa26-aeeb55b6679f"
      },
      "outputs": [],
      "source": [
        "%pip install scikit-learn tensorflow\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BcL_iwdLlYpb",
      "metadata": {
        "id": "BcL_iwdLlYpb"
      },
      "source": [
        "### Second step: We create the functions to be used throughout the project.\n",
        "\n",
        "The idea is, we need the code to be re-useable as much as possible because we will need to create multiple tests with the same functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea420e7f",
      "metadata": {
        "id": "ea420e7f"
      },
      "outputs": [],
      "source": [
        "def log_experiment(name, hyperparams, history, train_time, test_time, model=None):\n",
        "    print(f\"\\n--- {name} ---\")\n",
        "    print(\"Hyperparameters:\", hyperparams)\n",
        "    if model:\n",
        "        model.summary()\n",
        "        print(\"Total parameters:\", model.count_params())\n",
        "    print(\"Training time (s):\", train_time)\n",
        "    print(\"Testing time (s):\", test_time)\n",
        "    print(\"First 5 epochs accuracy:\", history['accuracy'][:5] if 'accuracy' in history else history[:5])\n",
        "    print(\"Final accuracy:\", history['accuracy'][-1] if 'accuracy' in history else history[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a35fc88",
      "metadata": {
        "id": "0a35fc88"
      },
      "outputs": [],
      "source": [
        "def load_preprocess_mnist():\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "    x_train = np.expand_dims(x_train, -1)\n",
        "    x_test = np.expand_dims(x_test, -1)\n",
        "    y_train_cat = keras.utils.to_categorical(y_train, 10)\n",
        "    y_test_cat = keras.utils.to_categorical(y_test, 10)\n",
        "    idx = np.random.permutation(len(x_train))\n",
        "    x_train, y_train, y_train_cat = x_train[idx], y_train[idx], y_train_cat[idx]\n",
        "    return (x_train, y_train, y_train_cat), (x_test, y_test, y_test_cat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ec9f08",
      "metadata": {
        "id": "f3ec9f08"
      },
      "outputs": [],
      "source": [
        "def build_ann(input_shape=(28,28,1), num_classes=10, hidden_units=128, activation='relu'):\n",
        "    model = keras.Sequential([\n",
        "        layers.Flatten(input_shape=input_shape),\n",
        "        layers.Dense(hidden_units, activation=activation),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_svm(x_train, y_train, x_test, y_test):\n",
        "    x_train_flat = x_train.reshape((x_train.shape[0], -1))\n",
        "    x_test_flat = x_test.reshape((x_test.shape[0], -1))\n",
        "    clf = svm.SVC()\n",
        "    start = time.time()\n",
        "    clf.fit(x_train_flat, y_train)\n",
        "    train_time = time.time() - start\n",
        "    start = time.time()\n",
        "    y_pred = clf.predict(x_test_flat)\n",
        "    test_time = time.time() - start\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    return clf, train_time, test_time, acc\n",
        "\n",
        "def build_cnn(input_shape=(28,28,1), num_classes=10,\n",
        "              conv_layers=2, filters=[32,64], kernel_size=3,\n",
        "              fc_layers=1, fc_units=[128], activation='relu',\n",
        "              dropout=None, dropout_rate=0.5):\n",
        "    model = keras.Sequential()\n",
        "    for i in range(conv_layers):\n",
        "        if i == 0:\n",
        "            model.add(layers.Conv2D(filters[i], (kernel_size, kernel_size), activation=activation, input_shape=input_shape))\n",
        "        else:\n",
        "            model.add(layers.Conv2D(filters[i], (kernel_size, kernel_size), activation=activation))\n",
        "        if i == 0:\n",
        "            model.add(layers.MaxPooling2D((2,2)))\n",
        "        if dropout and i in dropout:\n",
        "            model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.Flatten())\n",
        "    for i in range(fc_layers):\n",
        "        model.add(layers.Dense(fc_units[i], activation=activation))\n",
        "        if dropout and (conv_layers + i) in dropout:\n",
        "            model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea435f3",
      "metadata": {
        "id": "3ea435f3"
      },
      "outputs": [],
      "source": [
        "def train_model(model, x_train, y_train, x_test, y_test,\n",
        "                optimizer, loss, batch_size=64, epochs=10):\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "    start = time.time()\n",
        "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "                        validation_data=(x_test, y_test), verbose=0)\n",
        "    train_time = time.time() - start\n",
        "    start = time.time()\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    test_time = time.time() - start\n",
        "    return history.history, train_time, test_time, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aff2803",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aff2803",
        "outputId": "16abc826-7ff8-4b41-9f11-4e2880c24380"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train, y_train_cat), (x_test, y_test, y_test_cat) = load_preprocess_mnist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Falu6_eBrQq8",
      "metadata": {
        "id": "Falu6_eBrQq8"
      },
      "source": [
        "## Phase 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jLjH8QgmrTdJ",
      "metadata": {
        "id": "jLjH8QgmrTdJ"
      },
      "source": [
        "### STEP 1: Baseline ANN and SVM\n",
        "\n",
        "#### Description:\n",
        "Implemented two baseline models for benchmarking:\n",
        "- A simple **ANN** with one hidden dense layer.\n",
        "- A **Support Vector Machine (SVM)** using `sklearn` on flattened MNIST input.\n",
        "\n",
        "#### Hyperparameters:\n",
        "**ANN**\n",
        "- Optimizer: Adam\n",
        "- Activation: ReLU\n",
        "- Batch Size: 64\n",
        "- Epochs: 10\n",
        "\n",
        "**SVM**\n",
        "- Kernel: RBF\n",
        "- Training Samples: 10,000\n",
        "- Test Samples: 2,000\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0987e020",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "0987e020",
        "outputId": "4cc41c5f-c453-48a9-e4b9-04d86444b202"
      },
      "outputs": [],
      "source": [
        "ann = build_ann()\n",
        "history, train_time, test_time, test_acc = train_model(\n",
        "    ann, x_train, y_train_cat, x_test, y_test_cat,\n",
        "    optimizer=optimizers.Adam(), loss='categorical_crossentropy', batch_size=64, epochs=10)\n",
        "log_experiment(\"ANN Baseline\",\n",
        "               {\"optimizer\": \"Adam\", \"batch_size\": 64, \"epochs\": 10, \"activation\": \"relu\"},\n",
        "               history, train_time, test_time, ann)\n",
        "\n",
        "svm_clf, train_time, test_time, acc = train_svm(x_train[:10000], y_train[:10000], x_test[:2000], y_test[:2000])\n",
        "log_experiment(\"SVM Baseline\",\n",
        "               {\"kernel\": \"rbf\", \"train_samples\": 10000, \"test_samples\": 2000},\n",
        "               [acc], train_time, test_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tcKFsGxnrdPK",
      "metadata": {
        "id": "tcKFsGxnrdPK"
      },
      "source": [
        "#### Observations:\n",
        "- The ANN achieved high accuracy quickly and scaled well with the full dataset.\n",
        "- The SVM worked well on a reduced dataset but was significantly slower and memory-intensive.\n",
        "- ANN is more scalable and trainable in deep learning pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cw5gZuUirpCI",
      "metadata": {
        "id": "Cw5gZuUirpCI"
      },
      "source": [
        "### STEP 2: CNN Baseline\n",
        "\n",
        "#### Description:\n",
        "Implemented a basic CNN model with 3 convolutional layers and one FC layer. Started using:\n",
        "- ReLU activations\n",
        "- 2×2 MaxPooling after the first CNN layer\n",
        "- SGD optimizer\n",
        "\n",
        "#### Hyperparameters:\n",
        "- Conv Layers: 3 with filters [32, 64, 128]\n",
        "- FC Layers: 1 with 128 units\n",
        "- Batch Size: 64\n",
        "- Epochs: 15\n",
        "- Optimizer: SGD (learning rate: 0.01, momentum: 0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e90f112",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "4e90f112",
        "outputId": "38c9cfa2-ec83-4862-e06e-3fa04193860b"
      },
      "outputs": [],
      "source": [
        "cnn = build_cnn(conv_layers=3, filters=[32,64,128], fc_layers=1, fc_units=[128], activation='relu')\n",
        "history, train_time, test_time, test_acc = train_model(\n",
        "    cnn, x_train, y_train_cat, x_test, y_test_cat,\n",
        "    optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='categorical_crossentropy', batch_size=64, epochs=15)\n",
        "log_experiment(\"CNN Baseline\",\n",
        "               {\"optimizer\": \"SGD\", \"lr\": 0.01, \"momentum\": 0.9, \"batch_size\": 64, \"epochs\": 15, \"activation\": \"relu\"},\n",
        "               history, train_time, test_time, cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e4siXRqr53m",
      "metadata": {
        "id": "3e4siXRqr53m"
      },
      "source": [
        "#### Observations:\n",
        "- Model trained reliably and outperformed the ANN baseline.\n",
        "- Early signs of overfitting started to show but not significant.\n",
        "- Epochs around 15 gave a good balance between performance and time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LMXKLAvosDTe",
      "metadata": {
        "id": "LMXKLAvosDTe"
      },
      "source": [
        "### STEP 3: Learning Rate Study\n",
        "\n",
        "#### Description:\n",
        "Tested the effect of three different learning rates (0.01, 0.001, 0.0001) while keeping the architecture fixed. Used SGD optimizer.\n",
        "\n",
        "#### Hyperparameters:\n",
        "- Architecture: 3 CNN layers [32, 64, 128] + 1 FC layer [128]\n",
        "- Optimizer: SGD (momentum=0.9)\n",
        "- Batch Size: 64\n",
        "- Epochs: 15\n",
        "- Activation: ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c047730",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2c047730",
        "outputId": "8e0d3ffd-f64d-463f-efaa-b4fd23e206a6"
      },
      "outputs": [],
      "source": [
        "for lr in [0.01, 0.001, 0.0001]:\n",
        "    cnn = build_cnn(conv_layers=3, filters=[32,64,128], fc_layers=1, fc_units=[128], activation='relu')\n",
        "    history, train_time, test_time, test_acc = train_model(\n",
        "        cnn, x_train, y_train_cat, x_test, y_test_cat,\n",
        "        optimizer=optimizers.SGD(learning_rate=lr, momentum=0.9), loss='categorical_crossentropy', batch_size=64, epochs=15)\n",
        "    log_experiment(f\"CNN LR={lr}\",\n",
        "                   {\"optimizer\": \"SGD\", \"lr\": lr, \"momentum\": 0.9, \"batch_size\": 64, \"epochs\": 15, \"activation\": \"relu\"},\n",
        "                   history, train_time, test_time, cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lkziw9QasLAc",
      "metadata": {
        "id": "lkziw9QasLAc"
      },
      "source": [
        "#### Observations:\n",
        "- **LR = 0.01**\n",
        "  - Final Accuracy: **99.95%**\n",
        "  - First 5 Epochs: 93%, 98.2%, 98.8%, 99.1%, 99.3%\n",
        "  - Training Time: 79.32s\n",
        "  - Result: Fastest convergence and highest final accuracy.\n",
        "\n",
        "- **LR = 0.001**\n",
        "  - Final Accuracy: **99.28%**\n",
        "  - First 5 Epochs: 80.6%, 94.3%, 96.2%, 97.2%, 97.8%\n",
        "  - Training Time: 71.66s\n",
        "  - Result: More gradual learning, still high accuracy.\n",
        "\n",
        "- **LR = 0.0001**\n",
        "  - Final Accuracy: **95.22%**\n",
        "  - First 5 Epochs: 49.6%, 81.8%, 88.2%, 89.7%, 90.7%\n",
        "  - Training Time: 70.4s\n",
        "  - Result: Very slow convergence, underfitting.\n",
        "\n",
        "#### Conclusion:\n",
        "- **0.01** offered the best overall performance.\n",
        "- **0.001** was stable and a good fallback.\n",
        "- **0.0001** is too low for this task without more epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aFZPrPa-sOK7",
      "metadata": {
        "id": "aFZPrPa-sOK7"
      },
      "source": [
        "### STEP 4: CNN + FC Architecture Variations\n",
        "\n",
        "#### Description:\n",
        "Explored how varying the number of CNN and fully connected layers, and their sizes, affects training time, accuracy, and model complexity.\n",
        "\n",
        "#### Architecture Variants Tested:\n",
        "1. **Small Model**\n",
        "   - CNN Layers: 2 ([16, 32]), FC: 1 (64 units)\n",
        "   - Total Parameters: **~101,770**\n",
        "   - Final Accuracy: **99.45%**\n",
        "   - Training Time: ~39.65s\n",
        "\n",
        "2. **Larger Model**\n",
        "   - CNN Layers: 3 ([32, 64, 128]), FC: 1 (128 units)\n",
        "   - Total Parameters: **~1,421,194**\n",
        "   - Final Accuracy: **99.99%**\n",
        "   - Training Time: ~75–79s\n",
        "\n",
        "3. **Too Deep/Overfitting Model**\n",
        "   - Same CNN Layers, deeper FC stack (not shown, but inferred)\n",
        "   - Final Accuracy: dropped to **95.2%**\n",
        "   - Reason: learning rate too low, possible overfitting or underfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "debcc723",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "debcc723",
        "outputId": "c0fb1f1e-ab69-4047-b5d2-7945dd0abd96"
      },
      "outputs": [],
      "source": [
        "for conv_layers, fc_layers, filters, fc_units in [\n",
        "    (2, 1, [32,64], [128]),\n",
        "    (3, 2, [32,64,128], [256,128]),\n",
        "    (3, 1, [64,128,128], [256])\n",
        "]:\n",
        "    cnn = build_cnn(conv_layers=conv_layers, filters=filters, fc_layers=fc_layers, fc_units=fc_units, activation='relu')\n",
        "    history, train_time, test_time, test_acc = train_model(\n",
        "        cnn, x_train, y_train_cat, x_test, y_test_cat,\n",
        "        optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='categorical_crossentropy', batch_size=64, epochs=15)\n",
        "    log_experiment(f\"CNN {conv_layers}Conv {fc_layers}FC\",\n",
        "                   {\"optimizer\": \"SGD\", \"lr\": 0.01, \"momentum\": 0.9, \"batch_size\": 64, \"epochs\": 15, \"activation\": \"relu\",\n",
        "                    \"conv_layers\": conv_layers, \"fc_layers\": fc_layers, \"filters\": filters, \"fc_units\": fc_units},\n",
        "                   history, train_time, test_time, cnn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q2oKMuEQJCr3",
      "metadata": {
        "id": "Q2oKMuEQJCr3"
      },
      "source": [
        "\n",
        "#### Observations:\n",
        "- Increasing CNN layers and FC units improved accuracy — but at the cost of more parameters and training time.\n",
        "- Best trade-off found with 3 CNN layers and 1 FC layer (128 units).\n",
        "- Shallower networks still performed well and trained much faster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uQOh-Y8tJvJX",
      "metadata": {
        "id": "uQOh-Y8tJvJX"
      },
      "source": [
        "### STEP 5: Batch Size Study\n",
        "\n",
        "#### Description:\n",
        "Tested three different training batch sizes while keeping all other architecture and hyperparameters constant.\n",
        "\n",
        "#### Common Setup:\n",
        "- CNN: 3 Conv layers [32, 64, 128] with 2x2 max pooling after the first\n",
        "- FC: 1 Dense layer with 128 units\n",
        "- Optimizer: SGD (lr = 0.01, momentum = 0.9)\n",
        "- Epochs: 15\n",
        "- Activation: ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b13a1182",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b13a1182",
        "outputId": "f11730c4-3a8e-4b51-b678-d2755b30778d"
      },
      "outputs": [],
      "source": [
        "for batch_size in [64, 128, 192]:\n",
        "    cnn = build_cnn(conv_layers=3, filters=[32,64,128], fc_layers=1, fc_units=[128], activation='relu')\n",
        "    history, train_time, test_time, test_acc = train_model(\n",
        "        cnn, x_train, y_train_cat, x_test, y_test_cat,\n",
        "        optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='categorical_crossentropy', batch_size=batch_size, epochs=15)\n",
        "    log_experiment(f\"CNN BatchSize={batch_size}\",\n",
        "                   {\"optimizer\": \"SGD\", \"lr\": 0.01, \"momentum\": 0.9, \"batch_size\": batch_size, \"epochs\": 15, \"activation\": \"relu\"},\n",
        "                   history, train_time, test_time, cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XEeuR0FhJ5m7",
      "metadata": {
        "id": "XEeuR0FhJ5m7"
      },
      "source": [
        "\n",
        "#### Results:\n",
        "\n",
        " **Batch Size = 64**\n",
        "- Final Accuracy: **99.97%**\n",
        "- First 5 Epochs: [93.3%, 98.2%, 98.8%, 99.1%, 99.3%]\n",
        "- Training Time: **69.21s**\n",
        "- Testing Time: **0.99s**\n",
        "- Total Parameters: **1,421,194**\n",
        "\n",
        " **Batch Size = 128**\n",
        "- Final Accuracy: **99.86%**\n",
        "- First 5 Epochs: [90.5%, 97.7%, 98.4%, 98.9%, 99.1%]\n",
        "- Training Time: **70.06s**\n",
        "- Testing Time: **0.98s**\n",
        "- Total Parameters: **1,421,194**\n",
        "\n",
        " **Batch Size = 192**\n",
        "- Final Accuracy: **99.84%**\n",
        "- First 5 Epochs: [86.7%, 97.3%, 98.2%, 98.6%, 98.9%]\n",
        "- Training Time: **46.42s**\n",
        "- Testing Time: **0.92s**\n",
        "- Total Parameters: **1,421,194**\n",
        "\n",
        "---\n",
        "\n",
        "#### Observations:\n",
        "- **Batch size 64** gave the **highest accuracy** but took the longest to train.\n",
        "- **Larger batch sizes (128, 192)** were **faster** but slightly less accurate.\n",
        "- **192** offered a good balance of speed and performance if resources are constrained.\n",
        "- **64** is optimal if performance is more critical than training time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XOi9ztzxKCaH",
      "metadata": {
        "id": "XOi9ztzxKCaH"
      },
      "source": [
        "### STEP 6: Activation Function Study\n",
        "\n",
        "#### Description:\n",
        "Tested four activation functions — ReLU, Sigmoid, Tanh, and LeakyReLU — with the same CNN architecture to compare their effect on learning speed and accuracy.\n",
        "\n",
        "#### Common Setup:\n",
        "- CNN: 3 Conv layers [32, 64, 128] with 2x2 max pooling after the first\n",
        "- FC: 1 Dense layer with 128 units\n",
        "- Optimizer: SGD (lr = 0.01, momentum = 0.9)\n",
        "- Batch Size: 64\n",
        "- Epochs: 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c842183d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c842183d",
        "outputId": "e8e28bd1-0c3d-47a2-f728-05d257febe11"
      },
      "outputs": [],
      "source": [
        "for activation in ['relu', 'sigmoid', 'tanh']:\n",
        "    cnn = build_cnn(conv_layers=3, filters=[32,64,128], fc_layers=1, fc_units=[128], activation=activation)\n",
        "    history, train_time, test_time, test_acc = train_model(\n",
        "        cnn, x_train, y_train_cat, x_test, y_test_cat,\n",
        "        optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='categorical_crossentropy', batch_size=64, epochs=15)\n",
        "    log_experiment(f\"CNN Activation={activation}\",\n",
        "                   {\"optimizer\": \"SGD\", \"lr\": 0.01, \"momentum\": 0.9, \"batch_size\": 64, \"epochs\": 15, \"activation\": activation},\n",
        "                   history, train_time, test_time, cnn)\n",
        "\n",
        "# LeakyReLU\n",
        "cnn = keras.Sequential([\n",
        "    layers.Conv2D(32, (3,3), input_shape=(28,28,1)),\n",
        "    layers.LeakyReLU(alpha=0.1),\n",
        "    layers.Conv2D(64, (3,3)),\n",
        "    layers.LeakyReLU(alpha=0.1),\n",
        "    layers.Conv2D(128, (3,3)),\n",
        "    layers.LeakyReLU(alpha=0.1),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128),\n",
        "    layers.LeakyReLU(alpha=0.1),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "history, train_time, test_time, test_acc = train_model(\n",
        "    cnn, x_train, y_train_cat, x_test, y_test_cat,\n",
        "    optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='categorical_crossentropy', batch_size=64, epochs=15)\n",
        "log_experiment(\"CNN Activation=LeakyReLU\",\n",
        "               {\"optimizer\": \"SGD\", \"lr\": 0.01, \"momentum\": 0.9, \"batch_size\": 64, \"epochs\": 15, \"activation\": \"LeakyReLU\"},\n",
        "               history, train_time, test_time, cnn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i5rdwN32KciO",
      "metadata": {
        "id": "i5rdwN32KciO"
      },
      "source": [
        "\n",
        "#### Results:\n",
        "\n",
        " **ReLU**\n",
        "- Final Accuracy: **99.87%**\n",
        "- First 5 Epochs: [93.3%, 98.2%, 98.8%, 99.1%, 99.4%]\n",
        "- Training Time: **72.71s**\n",
        "- Testing Time: **1.00s**\n",
        "- Parameters: **1,421,194**\n",
        "\n",
        " **Sigmoid**\n",
        "- Final Accuracy: **89.48%**\n",
        "- First 5 Epochs: [10.4%, 10.3%, 10.2%, 10.5%, 10.3%]\n",
        "- Training Time: **73.84s**\n",
        "- Testing Time: **1.54s**\n",
        "- Parameters: **1,421,194**\n",
        "\n",
        " **Tanh**\n",
        "- Final Accuracy: **99.98%**\n",
        "- First 5 Epochs: [93.3%, 97.7%, 98.4%, 98.9%, 99.2%]\n",
        "- Training Time: **74.49s**\n",
        "- Testing Time: **1.08s**\n",
        "- Parameters: **1,421,194**\n",
        "\n",
        " **LeakyReLU**\n",
        "- Final Accuracy: **99.97%**\n",
        "- First 5 Epochs: [93.9%, 98.4%, 98.9%, 99.3%, 99.5%]\n",
        "- Training Time: **145.99s**\n",
        "- Testing Time: **1.36s**\n",
        "- Parameters: **2,076,554**\n",
        "\n",
        "---\n",
        "\n",
        "#### Observations:\n",
        "- **ReLU** and **LeakyReLU** performed exceptionally well; both achieved >99.7% accuracy.\n",
        "- **Tanh** matched their performance with slightly higher stability.\n",
        "- **Sigmoid** failed to learn effectively — likely due to **vanishing gradients**.\n",
        "- **LeakyReLU** was accurate but much slower due to increased model complexity.\n",
        "- Best trade-off: **ReLU** or **Tanh** for speed and performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zm8QtH7DK9wq",
      "metadata": {
        "id": "zm8QtH7DK9wq"
      },
      "source": [
        "### STEP 7: Optimizer Study\n",
        "\n",
        "#### Description:\n",
        "Compared the impact of three optimizers — **Adam**, **RMSProp**, and **SGD** — using the same CNN architecture to assess performance, convergence speed, and accuracy.\n",
        "\n",
        "#### Common Setup:\n",
        "- CNN: 3 Conv layers [32, 64, 128] with 2x2 max pooling after the first\n",
        "- FC: 1 Dense layer with 128 units\n",
        "- Activation: ReLU\n",
        "- Batch Size: 64\n",
        "- Epochs: 15\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb781a36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fb781a36",
        "outputId": "633755d6-1443-43ef-fa23-1658e120f6b6"
      },
      "outputs": [],
      "source": [
        "for opt_name, opt in [(\"Adam\", optimizers.Adam()), (\"RMSProp\", optimizers.RMSprop()), (\"SGD\", optimizers.SGD(learning_rate=0.01, momentum=0.9))]:\n",
        "    cnn = build_cnn(conv_layers=3, filters=[32,64,128], fc_layers=1, fc_units=[128], activation='relu')\n",
        "    history, train_time, test_time, test_acc = train_model(\n",
        "        cnn, x_train, y_train_cat, x_test, y_test_cat,\n",
        "        optimizer=opt, loss='categorical_crossentropy', batch_size=64, epochs=15)\n",
        "    log_experiment(f\"CNN Optimizer={opt_name}\",\n",
        "                   {\"optimizer\": opt_name, \"batch_size\": 64, \"epochs\": 15, \"activation\": \"relu\"},\n",
        "                   history, train_time, test_time, cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mNqUhXiyLCOc",
      "metadata": {
        "id": "mNqUhXiyLCOc"
      },
      "source": [
        "\n",
        "#### Results:\n",
        "\n",
        " **Adam**\n",
        "- Final Accuracy: **99.84%**\n",
        "- First 5 Epochs: [96.3%, 98.9%, 99.2%, 99.4%, 99.6%]\n",
        "- Training Time: **77.17s**\n",
        "- Testing Time: **1.12s**\n",
        "- Parameters: **1,421,194**\n",
        "\n",
        " **RMSProp**\n",
        "- Final Accuracy: **99.96%**\n",
        "- First 5 Epochs: [96.1%, 98.8%, 99.3%, 99.5%, 99.7%]\n",
        "- Training Time: **70.34s**\n",
        "- Testing Time: **1.28s**\n",
        "- Parameters: **1,421,194**\n",
        "\n",
        " **SGD**\n",
        "- Final Accuracy: **99.99%**\n",
        "- First 5 Epochs: [92.9%, 98.3%, 98.8%, 99.1%, 99.4%]\n",
        "- Training Time: **72.65s**\n",
        "- Testing Time: **0.92s**\n",
        "- Parameters: **1,421,194**\n",
        "\n",
        "---\n",
        "\n",
        "#### Observations:\n",
        "- **SGD** surprisingly achieved the **highest final accuracy**, though it required more epochs to converge.\n",
        "- **Adam** and **RMSProp** both converged faster in early epochs and performed very well overall.\n",
        "- **RMSProp** was the most balanced in terms of speed and performance.\n",
        "- All three optimizers were highly effective for this task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oDF3sGOrLXpl",
      "metadata": {
        "id": "oDF3sGOrLXpl"
      },
      "source": [
        "### STEP 8: Dropout Study\n",
        "\n",
        "#### Description:\n",
        "Tested the effect of adding **Dropout** layers to reduce overfitting. Two dropout layers were inserted:\n",
        "- After the second Conv2D layer\n",
        "- After the fully connected (Dense) layer\n",
        "\n",
        "Two dropout rates were tested: **0.3** and **0.5**, using the same architecture and training configuration.\n",
        "\n",
        "#### Common Setup:\n",
        "- CNN: 3 Conv layers [32, 64, 128] + MaxPooling\n",
        "- FC: Dense(128)\n",
        "- Activation: ReLU\n",
        "- Optimizer: Adam\n",
        "- Batch Size: 64\n",
        "- Epochs: 15\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc88a71b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dc88a71b",
        "outputId": "ce83fcef-6e1f-455a-b682-e4f2742d79a0"
      },
      "outputs": [],
      "source": [
        "for dropout_rate in [0.3, 0.5]:\n",
        "    cnn = build_cnn(conv_layers=3, filters=[32,64,128], fc_layers=1, fc_units=[128], activation='relu', dropout=[1,3], dropout_rate=dropout_rate)\n",
        "    history, train_time, test_time, test_acc = train_model(\n",
        "        cnn, x_train, y_train_cat, x_test, y_test_cat,\n",
        "        optimizer=optimizers.Adam(), loss='categorical_crossentropy', batch_size=64, epochs=15)\n",
        "    log_experiment(f\"CNN Dropout={dropout_rate}\",\n",
        "                   {\"optimizer\": \"Adam\", \"batch_size\": 64, \"epochs\": 15, \"activation\": \"relu\", \"dropout_rate\": dropout_rate},\n",
        "                   history, train_time, test_time, cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BZTDdgICLp6E",
      "metadata": {
        "id": "BZTDdgICLp6E"
      },
      "source": [
        "\n",
        "#### Results:\n",
        "\n",
        "**Dropout Rate = 0.3**\n",
        "- Final Accuracy: **99.76%**\n",
        "- First 5 Epochs: [95.2%, 98.4%, 98.8%, 99.1%, 99.2%]\n",
        "- Training Time: **75.21s**\n",
        "- Testing Time: **1.00s**\n",
        "- Parameters: **1,421,194**\n",
        "\n",
        "**Dropout Rate = 0.5**\n",
        "- Final Accuracy: **99.46%**\n",
        "- First 5 Epochs: [93.2%, 97.6%, 98.2%, 98.5%, 98.7%]\n",
        "- Training Time: **79.40s**\n",
        "- Testing Time: **1.02s**\n",
        "- Parameters: **1,421,194**\n",
        "\n",
        "---\n",
        "\n",
        "#### Observations:\n",
        "- Dropout **reduced overfitting** and slightly decreased final accuracy.\n",
        "- **0.3 dropout rate** preserved more performance while offering regularization.\n",
        "- **0.5 rate** led to some underfitting, slower convergence, and lower accuracy.\n",
        "- Best balance: **Dropout(0.3)** — especially after Conv and FC layers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YglK8E0VMQ9U",
      "metadata": {
        "id": "YglK8E0VMQ9U"
      },
      "source": [
        "## Phase 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SFm1XTPsL5ys",
      "metadata": {
        "id": "SFm1XTPsL5ys"
      },
      "source": [
        "### Final Model Summary\n",
        "\n",
        "#### Best Performing Configuration:\n",
        "\n",
        "| Component         | Configuration                                |\n",
        "|------------------|----------------------------------------------|\n",
        "| Architecture     | CNN (3 Conv layers + MaxPooling + FC layer)  |\n",
        "| Conv Filters     | [32, 64, 128]                                 |\n",
        "| FC Units         | 128                                           |\n",
        "| Activation       | **Tanh** or **ReLU**                         |\n",
        "| Optimizer        | **SGD** (best final accuracy) or RMSProp     |\n",
        "| Learning Rate    | 0.01                                          |\n",
        "| Batch Size       | 64                                            |\n",
        "| Epochs           | 15                                            |\n",
        "| Dropout          | **0.3** after Conv2D and Dense layers        |\n",
        "\n",
        "---\n",
        "\n",
        "#### Best Results:\n",
        "- **Final Accuracy**: **99.99%** (SGD)\n",
        "- **Fastest Convergence**: Adam & RMSProp\n",
        "- **Most Balanced**: RMSProp + Tanh\n",
        "- **Best Regularization**: Dropout(0.3)\n",
        "- **Total Parameters**: ~1.42 million\n",
        "\n",
        "---\n",
        "\n",
        "#### Why It Worked Best:\n",
        "- A three-layer CNN captured **rich hierarchical features** from MNIST digits.\n",
        "- **Moderate FC size (128)** kept overfitting in check while maintaining capacity.\n",
        "- **ReLU** or **Tanh** provided efficient and stable learning.\n",
        "- **SGD with lr=0.01** converged slowly but delivered **perfect accuracy**.\n",
        "- Dropout at **0.3** offered regularization without hurting performance.\n",
        "- **Batch size of 64** ensured good generalization and training stability.\n",
        "\n",
        "---\n",
        "\n",
        "#### Recommendations:\n",
        "- Use **RMSProp + Tanh** for fast and stable training.\n",
        "- Use **SGD** when aiming for maximum accuracy and interpretability.\n",
        "- Prefer dropout rate **0.3** for low risk of overfitting.\n",
        "\n",
        "#### Notes:\n",
        "- All models used **cross-entropy loss**.\n",
        "- Activation and softmax layers were **not counted** as structural layers, per guidelines.\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
